import os
import time
import requests
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from webdriver_manager.chrome import ChromeDriverManager

# .env ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()
NOTION_API_KEY = os.environ.get("NOTION_API_KEY")
NOTION_DATABASE_ID = os.environ.get("NOTION_DATABASE_ID")
NOTE_SEARCH_KEYWORD = os.environ.get("NOTE_SEARCH_KEYWORD")

# Notion APIã®ãƒ˜ãƒƒãƒ€
HEADERS = {
    "Authorization": f"Bearer {NOTION_API_KEY}",
    "Notion-Version": "2022-06-28",
    "Content-Type": "application/json",
}

# Seleniumã®è¨­å®šï¼ˆWebDriverManagerã‚’ä½¿ç”¨ï¼‰
options = Options()
options.add_argument("--headless")  # ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")

service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=options)

def scroll_down():
    """
    ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦è¨˜äº‹ã‚’ã™ã¹ã¦èª­ã¿è¾¼ã‚€
    """
    for _ in range(5):  # 5å›ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼ˆå¿…è¦ã«å¿œã˜ã¦å›æ•°èª¿æ•´ï¼‰
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.PAGE_DOWN)
        time.sleep(1)  # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã«å°‘ã—å¾…æ©Ÿ

def scrape_note_articles(keyword):
    """
    æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ note.com ã®æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‚’é–‹ãã€
    è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ãƒ»URLãƒ»è‘—è€…ãƒ»ã„ã„ã­æ•°ã‚’å–å¾—ã—ã¦è¿”ã™ã€‚
    """
    search_url = f"https://note.com/search?q={keyword}&context=note&mode=search"
    driver.get(search_url)
    time.sleep(5)  # ãƒšãƒ¼ã‚¸ã®ãƒ­ãƒ¼ãƒ‰ã‚’å¾…æ©Ÿ

    # ãƒ‡ãƒãƒƒã‚°ç”¨: HTMLå…¨ä½“ã‚’å‡ºåŠ›ï¼ˆå…ˆé ­2000æ–‡å­—ï¼‰
    # soup = BeautifulSoup(driver.page_source, "html.parser")
    # print("===== [DEBUG] ãƒšãƒ¼ã‚¸HTMLã®ä¸€éƒ¨ã‚’å‡ºåŠ› =====")
    # print(soup.prettify()[:2000])
    # print("===================================")

    # è¨˜äº‹ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, ".m-largeNoteWrapper__card"))
        )
    except:
        print("âš ï¸ æ¤œç´¢çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return []

    # ååˆ†ãªè¨˜äº‹ã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
    scroll_down()

    # å†åº¦HTMLã‚’å–å¾—
    soup = BeautifulSoup(driver.page_source, "html.parser")

    articles_data = []
    cards = soup.select(".m-largeNoteWrapper__card")
    if not cards:
        print("âš ï¸ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return []

    for card in cards:
        # ã‚¿ã‚¤ãƒˆãƒ«
        title_tag = card.select_one("h3.m-noteBodyTitle__title")
        title = title_tag.get_text(strip=True) if title_tag else "No Title"

        # URL
        a_tag = card.select_one("a.m-largeNoteWrapper__link")
        if not a_tag:
            continue
        relative_url = a_tag.get("href", "")
        if relative_url.startswith("/"):
            url = "https://note.com" + relative_url
        else:
            url = relative_url

        # è‘—è€…
        author_tag = card.select_one(".o-largeNoteSummary__userName")
        author = author_tag.get_text(strip=True) if author_tag else "ä¸æ˜"

        # ã„ã„ã­æ•°
        like_tag = card.select_one("span.pl-2.text-sm.text-text-secondary")
        like_count_str = like_tag.get_text(strip=True) if like_tag else "0"
        like_count_str = like_count_str.replace(",", "")  # ã‚«ãƒ³ãƒé™¤å»
        like_count = int(like_count_str) if like_count_str.isdigit() else 0

        articles_data.append({
            "title": title,
            "url": url,
            "author": author,
            "like_count": like_count
        })

    return articles_data

def create_notion_page(article, keyword):
    """
    Notion APIã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜äº‹ã‚’ç™»éŒ²
    è¿½åŠ : 'æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰'ç”¨ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ "Keyword"
    """
    url = "https://api.notion.com/v1/pages"

    payload = {
        "parent": {"database_id": NOTION_DATABASE_ID},
        "properties": {
            "Name": {
                "title": [
                    {"type": "text", "text": {"content": article["title"]}}
                ]
            },
            "URL": {"url": article["url"]},
            "AUTHOR": {
                "rich_text": [
                    {"type": "text", "text": {"content": article["author"]}}
                ]
            },
            "LIKE": {"number": article["like_count"]},
            # â˜… ã“ã“ã§æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã‚’ Notion ã«æ›¸ãè¾¼ã‚€
            "Keyword": {
                "rich_text": [
                    {
                        "type": "text",
                        "text": {"content": keyword}
                    }
                ]
            }
        }
    }

    res = requests.post(url, headers=HEADERS, json=payload)

    if res.status_code in [200, 201]:
        print(f"âœ… Notionç™»éŒ²æˆåŠŸ: {article['title']}")
    else:
        print(f"âŒ Notionç™»éŒ²å¤±æ•—: {article['title']}")
        print("Status Code:", res.status_code)
        print("Response:", res.text)

def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†:
    1. Noteã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    2. Notionã«ãƒ‡ãƒ¼ã‚¿ç™»éŒ² (æ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ )
    """
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€{NOTE_SEARCH_KEYWORD}ã€ã§noteè¨˜äº‹ã‚’æ¤œç´¢ä¸­...")
    articles = scrape_note_articles(NOTE_SEARCH_KEYWORD)
    print(f"âœ… è¨˜äº‹ {len(articles)} ä»¶å–å¾—")

    for article in articles:
        # create_notion_page() ã« æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¸¡ã™
        create_notion_page(article, NOTE_SEARCH_KEYWORD)

    driver.quit()
    print("ğŸ‰ å…¨è¨˜äº‹ã®ç™»éŒ²å®Œäº†!")

if __name__ == "__main__":
    main()
